{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mlpc.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNsJIli5M4VRcG9jc0AE537",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anniebbii/bork/blob/master/mlpc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThAMyermEL_a"
      },
      "source": [
        "import pandas as pd\n",
        "import xgboost as xgb\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# take care of missing values?\n",
        "# identify outliers? (missed comma, extra 0 etc.)\n",
        "# standardize inputs?\n",
        "\n",
        "# data before cleaning:\n",
        "# y is names ['Bob' 'Atsuto' 'Jörg' ' ooh'] only one ' ooh'\n",
        "# x5 is true/false ['False' 'True' '?' 'F' nan], two '?', one 'F', one nan\n",
        "# x6 is letters ['F' 'E' 'A' 'D' 'B' 'Fx' 'C' '-0.46960' nan] only one '-0.4...', one nan\n",
        "\n",
        "# import data as pandas dataframe\n",
        "#df = pd.read_csv(\"TrainOnMe.csv\")\n",
        "url = 'https://raw.githubusercontent.com/anniebbii/mlpc/main/TrainOnMe.csv'\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# remove rows with weird entries since they're few and we got enough data\n",
        "df = df.drop(df[df.y == ' ooh'].index)\n",
        "df = df.drop(df[df.x5 == '?'].index)\n",
        "df = df.drop(df[df.x5 == 'F'].index)\n",
        "df = df.drop(df[df.x6 == '-0.46960'].index)\n",
        "#df = df.dropna(axis=0, how='any')  # drop rows containing any null values\n",
        "\n",
        "# encode categorical variables\n",
        "df = df.replace('True', 1)\n",
        "df = df.replace('False', 0)\n",
        "df = df.replace('F', -1)\n",
        "df = df.replace('Fx', 0)\n",
        "df = df.replace('E', 1)\n",
        "df = df.replace('D', 2)\n",
        "df = df.replace('C', 3)\n",
        "df = df.replace('B', 4)\n",
        "df = df.replace('A', 5)\n",
        "# encode labels\n",
        "df = df.replace('Atsuto', 0)\n",
        "df = df.replace('Bob', 1)\n",
        "df = df.replace('Jörg', 2)\n",
        "\n",
        "# change column types\n",
        "df = df.astype({\"x1\": float, \"x2\": float})\n",
        "\n",
        "#print(df.head())\n",
        "#print(df.info())\n",
        "#print(df.describe())\n",
        "\n",
        "# separate features and labels\n",
        "# separate train/validation and test data\n",
        "X = df.iloc[:, 2:]\n",
        "y = df.iloc[:, 1]\n",
        "\n",
        "X_ttest = df.iloc[-100:, 2:]\n",
        "y_ttest = df.iloc[-100:, 1]\n",
        "\n",
        "# order in a dmatrix for xgboost\n",
        "dm = xgb.DMatrix(data=X, label=y)\n",
        "\n",
        "# split into train and validation data for cross validation\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=123)\n",
        "\n",
        "# create an xgboost model\n",
        "xg_model = xgb.XGBClassifier(objective ='multi:softprob', colsample_bytree = 0.6000000000000001, learning_rate = 0.7919793520294113, max_depth = 3, alpha = 3, n_estimators = 22)\n",
        "#alpha=3, colsample_bytree=0.6000000000000001, learning_rate=0.7919793520294113, max_depth=3, n_estimators=22\n",
        "# fit model to data\n",
        "xg_model.fit(X_train, y_train)\n",
        "\n",
        "# predict on validation dataset using trained model\n",
        "preds = xg_model.predict(X_val)\n",
        "\n",
        "# calculate accuracy\n",
        "accuracy = accuracy_score(y_val, preds)\n",
        "print(\"model accuracy\", accuracy)\n",
        "\n",
        "params = {\"objective\": \"multi:softprob\", 'colsample_bytree': 0.5, 'learning_rate': 0.8, 'max_depth': 5, 'alpha': 4, \"num_class\": 3}\n",
        "\n",
        "cv_results = xgb.cv(dtrain=dm, params=params, nfold=10,\n",
        "                    num_boost_round=3000, early_stopping_rounds=10, metrics=\"merror\", as_pandas=True, seed=123)\n",
        "\n",
        "#print(cv_results.tail(1))\n",
        "\n",
        "# accuracy is 87%\n",
        "# train the model on whole training set\n",
        "xg_model.fit(X, y)\n",
        "\n",
        "\n",
        "# clean test data\n",
        "#for key in test:\n",
        "#    print(test[key].unique())\n",
        "# seems pretty clean\n",
        "\n",
        "#print(test.info()) # it's freeking purrfect\n",
        "#X_test = test.iloc[:, 1:]\n",
        "#print(X_test.head())\n",
        "\n",
        "\n",
        "from scipy import stats\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, KFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "x = X\n",
        "\n",
        "clf_xgb = XGBClassifier(objective = 'multi:softprob')\n",
        "param_dist = {'learning_rate': [.25],\n",
        "              'max_depth': [5],\n",
        "              'alpha': [3],\n",
        "              'n_estimators': [20, 25, 30],\n",
        "              'colsample_bytree': [.5, .55, .6]}\n",
        "clf = GridSearchCV(clf_xgb, param_grid = param_dist, scoring = 'accuracy', error_score = 0, verbose = 0, n_jobs = -1)\n",
        "\n",
        "numFolds = 10\n",
        "folds = KFold(n_splits = numFolds, shuffle = True)\n",
        "\n",
        "estimators = []\n",
        "results = np.zeros(len(X))\n",
        "score = 0.0\n",
        "for train_index, test_index in folds.split(X):                                                  # for each k\n",
        "    X_train, X_test = X.iloc[train_index,:], X.iloc[test_index,:]                               # create training set T_k\n",
        "    y_train, y_test = y.iloc[train_index].values.ravel(), y.iloc[test_index].values.ravel()     # and validation set V_k\n",
        "    clf.fit(X_train, y_train)                                                                   # do grid search CV\n",
        "\n",
        "    estimators.append(clf.best_estimator_)\n",
        "    results[test_index] = clf.predict(X_test)\n",
        "    score += accuracy_score(y_test, results[test_index])\n",
        "score /= numFolds\n",
        "\n",
        "print(\"score:\", score)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvY6E_RWNeSf"
      },
      "source": [
        "for i in estimators:\n",
        "  print(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjcbeNgcM-5H"
      },
      "source": [
        "scores = []\n",
        "for model in estimators:\n",
        "    numFolds = 10\n",
        "    folds = KFold(n_splits = numFolds, shuffle = True)\n",
        "    results = np.zeros(len(X))\n",
        "    score = []\n",
        "    for train_index, test_index in folds.split(X):                                                  # for each k\n",
        "        X_train, X_test = X.iloc[train_index,:], X.iloc[test_index,:]                               # create training set T_k\n",
        "        y_train, y_test = y.iloc[train_index].values.ravel(), y.iloc[test_index].values.ravel()     # and validation set V_k\n",
        "        model.fit(X_train, y_train)                                                                   # do grid search CV\n",
        "        results[test_index] = model.predict(X_test)\n",
        "        score.append(accuracy_score(y_test, results[test_index]))\n",
        "    scores.append([np.mean(score), np.std(score)])\n",
        "\n",
        "for i in scores:\n",
        "  print(i)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BUmWqjqS3W9"
      },
      "source": [
        "clf_xgb = XGBClassifier(objective = 'multi:softprob')\n",
        "param_dist = {'learning_rate': [.8],\n",
        "              'max_depth': [5],\n",
        "              'alpha': [5],\n",
        "              'n_estimators': [25],\n",
        "              'colsample_bytree': [0.4, 0.5, 0.6]}\n",
        "\n",
        "clf = GridSearchCV(clf_xgb, param_grid = param_dist, scoring = 'accuracy',  # create grid-search with 5-fold CV\n",
        "                   error_score = 0, verbose = 0, n_jobs = -1)\n",
        "\n",
        "numFolds = 10\n",
        "folds = KFold(n_splits = numFolds, shuffle = True)\n",
        "\n",
        "scores = []\n",
        "for train_index, test_index in folds.split(X):                                                  # for each k\n",
        "    X_train, X_test = X.iloc[train_index,:], X.iloc[test_index,:]                               # create training set T_k\n",
        "    y_train, y_test = y.iloc[train_index].values.ravel(), y.iloc[test_index].values.ravel()     # and validation set V_k\n",
        "    clf.fit(X_train, y_train)                                                                   # do grid search CV\n",
        "\n",
        "    estimator = clf.best_estimator_                                                             # then pick best model\n",
        "    preds = estimator.predict(X_test)                                                                 # and test its accuracy\n",
        "    score = accuracy_score(y_test, preds)                                                       # on validation data\n",
        "    scores.append(score)\n",
        "\n",
        "np.mean(scores)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}